# kubernetes構築その2
ちゃんと使える環境を作りたい

## この後やってみたいこと
- kubetnetesの機能を試したいからkubeadmを使ってみる
- 公式を参考にソースからbuildする(https://kubernetes.io/docs/setup/building-from-source/)

## 対象サーバ


| ホスト名          | IPアドレス       |
|:-----------       |:------------     |
|zssg-k8sma01.local |192.168.2.180/24  |
|zssg-k8swk01.local |192.168.2.181/24  |
|zssg-k8swk02.local |192.168.2.182/24  |

全サーバでyum update実施済

## kubeadm
## Installing kubeadm

### 参考
公式(https://kubernetes.io/docs/setup/independent/install-kubeadm/)


初期設定はAnsibleで  

### 参考
https://thinkit.co.jp/article/9472  

ansibleインストール
~~~
yum install -y epel-release 
yum install -y ansible --enablerepo=epel-testing

~~~

SSHキーの送付
~~~
ssh-keygen 
ssh-copy-id -i ~/.ssh/id_rsa.pub <targetのIP>

# 以下にsshキーが作成される
~/.ssh/
~~~

/etc/ansible/hostsへのtarget IP追記
~~~
vi /etc/ansible/hosts

---略---

192.168.2.180
192.168.2.181
192.168.2.182
~~~


ansibleのテスト
~~~
ansible <targetのIP> -m ping
~~~

### inventory
~~~
zssg-k8sma01 ansible_ssh_host=192.168.2.180
zssg-k8swk01 ansible_ssh_host=192.168.2.181
zssg-k8swk02 ansible_ssh_host=192.168.2.182

[servers]
zssg-k8sma01
zssg-k8swk01
zssg-k8swk02

[master]
zssg-k8sma01

[node]
zssg-k8swk01
zssg-k8swk02

~~~

### setup-playbook.yml
~~~
- hosts: servers
  tasks:
   # - name: ping
   #  ping:

   # - name: yum_update
   #   yum: name=* state=latest

   # - name: hosts_copy
   #  copy:
   #    dest: /etc/hosts
   #    src: ./hosts

   - name: hosts_add
     lineinfile:
        dest='/etc/hosts'
        line={{ item }}
     with_items:
       - '192.168.2.180   zssg-k8sma01'
       - '192.168.2.181   zssg-k8swk01'
       - '192.168.2.182   zssg-k8swk02'
       - '192.168.2.180   zssg-k8sma01.local'
       - '192.168.2.181   zssg-k8swk01.local'
       - '192.168.2.182   zssg-k8swk02.local'


   - name: bash-completion_install
     yum: name=bash-completion state=latest

   - name: tcpdump_install
     yum: name=tcpdump state=latest

   - name: chrony_install
     yum: name=chrony state=latest

   - name: wget_install
     yum: name=wget state=latest

   - name: git_install
     yum: name=git state=latest

   - name: epel-release_install
     yum: name=epel-release state=latest

   - name: docker_install
     yum: name=docker state=latest

   - name: docker_service_start
     service: name=docker state=started

   - name: docker_service_enable
     service: name=docker enabled=yes

   - name: firewalld_service_start
     service: name=firewalld state=stopped

   - name: firewalld_service_enable
     service: name=firewalld enabled=no

   - name: stop_SELinux
     selinux: state=disabled

   # - name: repository_set
   #   copy:
   #     dest: /etc/yum.repos.d/
   #     src: ./virt7-docker-common-release.repo

   - name: repository_set
     copy:
       dest: /etc/yum.repos.d/
       src: ./kubernetes.repo

   # - name: kubernetes_install
   #   yum: name=kubernetes enablerepo=virt7-docker-common-release state=latest

   - name: kubelet_install
     yum: name=kubelet state=latest

   - name: kubeadm_install
     yum: name=kubeadm state=latest
     
   - name: kubectl_install
     yum: name=kubectl state=latest

   - name: kubelet_service_start
     service: name=kubelet state=started

   - name: kubelet_service_enable
     service: name=kubelet enabled=yes

   # - name: flannel_install
   #   yum: name=flannel enablerepo=virt7-docker-common-release state=latest

   - name: iptables_install
     yum: name=iptables-services state=latest

   - name: iptables_service_disable
     service: name=iptables enabled=no
     
   - name: iptables_set
     copy:
       dest: /etc/sysctl.d
       src: ./k8s.conf

     - name: sysctl
       command: sysctl --system

~~~
※RHEL系の場合、iptablesがバイパスされてトラフィックが変にルーティングされる問題があるため、  
  net.bridge.bridge-nf-call-iptablesを1にセットしておく必要がある。  
/etc/sysctl.d/k8s.conf
~~~
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
~~~



playbookを実行したら一回再起動
~~~
reboot
~~~

## Using kubeadm to Create a Cluster

### 参考
https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/  
https://www.kaitoy.xyz/2017/10/21/build-kubernetes-cluster-by-kubeadm/  

## Master

kubeadmを入れたら改めてyum update  

ebtablesとethtoolをインストール
~~~
# yum install -y ebtables ethtool
→すでに入ってた
~~~


masterの構築
~~~
# kubeadm init --apiserver-advertise-address=192.168.2.180 --pod-network-cidr=10.244.0.0/16

※--apiserver-advertise-address=192.168.2.180：apiserverのIPアドレス
※--pod-network-cidr=10.244.0.0/16：CNI(flannelとか)で使うNW

[init] Using Kubernetes version: v1.9.3
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks.
        [WARNING FileExisting-crictl]: crictl not found in system path
[preflight] Some fatal errors occurred:
        [ERROR Swap]: running with swap on is not supported. Please disable swap
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`


~~~

swapがONだとコケるらしい

~~~
# vi /etc/fstab

以下の行をコメントアウト
# /dev/mapper/cl_zssg--k8sma01-swap swap                    swap    defaults        0 0

~~~

ちなみに以下で無効化すると、OS再起動で再度ONになってしまう
~~~
swapoff -a

~~~
swap状態確認

~~~
swapon -s
~~~


改めて実行

~~~
# kubeadm init --apiserver-advertise-address=192.168.2.180 --pod-network-cidr=10.244.0.0/16

[init] Using Kubernetes version: v1.9.3
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks.
        [WARNING FileExisting-crictl]: crictl not found in system path
[preflight] Starting the kubelet service
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [zssg-k8sma01.local kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.2.180]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "scheduler.conf"
[controlplane] Wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[controlplane] Wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests".
[init] This might take a minute or longer if the control plane images have to be pulled.
[apiclient] All control plane components are healthy after 27.001902 seconds
[uploadconfig]?Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[markmaster] Will mark node zssg-k8sma01.local as master by adding a label and a taint
[markmaster] Master zssg-k8sma01.local tainted and labelled with key/value: node-role.kubernetes.io/master=""
[bootstraptoken] Using token: f4d188.a2e330453d1c4bc2
[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstraptoken] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: kube-dns
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join --token f4d188.a2e330453d1c4bc2 192.168.2.180:6443 --discovery-token-ca-cert-hash sha256:7d9a9e8d8b864c5ba94197447f9ca3dfc3bb17bf13b24c56c63757915341dd6f

~~~

kubeadm initをやり直すとき

~~~
kubeadm reset
~~~

kubectlの設定
~~~
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

export KUBECONFIG=/etc/kubernetes/admin.conf
~~~
これで一応kubectlでapi-serverと通信できるようになった
~~~
kubectl version
kubectl get nodes

~~~

CNI(flannel)の設定
~~~
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
~~~
しばらくするとkube-dnsが自動で起動する(以下で確認)

~~~
kubectl get pods --all-namespaces
~~~



### master-setup-playbook.yml

~~~
- hosts: master
  tasks:

     - name: reset_master
       command: kubeadm reset

### swapをoffにするを追加

     - name: init_master
       command: kubeadm init --apiserver-advertise-address=192.168.2.180 --pod-network-cidr=10.244.0.0/16

### kubectlの設定を追加

     - name: flannel_install
       command: kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
     
  #  - name: etcd_install
  #    yum: name=etcd enablerepo=virt7-docker-common-release state=latest

  #  - name: openssl
  #    command: openssl genrsa -out /etc/kubernetes/serviceaccount.key 2048

  #  - name: etcd_service_restart
  #    service: name=etcd state=restarted
~~~

## Node
nodeの設定では先に作ったclusterにnodeを参加させる

まずmasterと同じくswapを無効にする

~~~
# vi /etc/fstab

以下の行をコメントアウト
# /dev/mapper/cl_zssg--k8sma01-swap swap                    swap    defaults        0 0

~~~

~~~
kubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<hash>
kubeadm join --token f4d188.a2e330453d1c4bc2 192.168.2.180:6443 --discovery-token-ca-cert-hash sha256:7d9a9e8d8b864c5ba94197447f9ca3dfc3bb17bf13b24c56c63757915341dd6f
~~~


## kubenetesの動作確認
nginを立ててみる  
参考  
https://qiita.com/tkusumi/items/01cd18c59b742eebdc6a  
https://qiita.com/suzukihi724/items/5e16855c3941e629ff70

nginx-test-svc.yml
~~~
apiVersion: v1
kind: Service
metadata:
  name: nginx-test
spec:
  type: NodePort  ### ホストとコンテナIPの紐づけをする設定
  ports:
  - port: 8080  ### ローカル(kubernetesの外)からPodへ接続用ポート
    targetPort: 80  ### POD内のコンテナのサービスポート(containerPortで指定してる値)
    nodePort: 30000  ### serveceが外部に公開するポート
  selector:
    app: nginx-test
  clusterIP:
~~~
サービスの各パラメータ  



nginx-deployment.yml
~~~
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: nginx-test
spec:
  selector:
    matchLabels:
      app: nginx-test
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: nginx-test
    spec:
      containers:
      - name: nginx-test
        image: nginx:latest
        ports:
        - containerPort: 80
          name: nginx-test
~~~


## ダッシュボードを入れる

参考  
https://github.com/kubernetes/dashboard  

~~~
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
kubectl proxy
~~~
→無理！  
kubectl proxyのみだとlocalhostからしかアクセスできない

別のやり方を考える  
参考  
https://github.com/kubernetes/dashboard/wiki

~~~
git clone https://github.com/kubernetes/dashboard.git

~~~

これでできた！
~~~
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
kubectl proxy --address='<master ip>' --accept-hosts=
~~~
ブラウザで以下にアクセス
~~~
http://<master ip>:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/
~~~

### ダッシュボードへの認証方法変更


~~~
# ------------------- Dashboard Deployment ------------------- #

kind: Deployment
apiVersion: apps/v1beta2
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kube-system
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      containers:
      - name: kubernetes-dashboard
        image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3
        ports:
        - containerPort: 8443
          protocol: TCP
        args:
          - --auto-generate-certificates
          - --authentication-mode=basic  ### ←here
          # Uncomment the following line to manually specify Kubernetes API server Host
          # If not specified, Dashboard will attempt to auto discover the API server and connect
          # to it. Uncomment only if the default does not work.
          # - --apiserver-host=http://my-address:port
        volumeMounts:

~~~
→ログオンできない・・・


## namespace

### namespaceの確認
~~~
kubectl get namespaces
~~~

### 今のnamespace確認
~~~
kubectl config view | grep namespace
~~~

### namespaceの変更
~~~
kubectl config set-context $(kubectl config current-context) --namespace=<namespace>

configを書き換えるのと同じ
vi ~/.kube/config
→namespaceを任意のnamespaceに変更
~~~

### namespaceの作成
~~~
kubectl create ns <namespace>
~~~


## メモ

### エイリアス設定

~/.bashrc
~~~
alias kc='kubectl'
~~~

~/.bash_profile
~~~
source ~/.bashrc
~~~

